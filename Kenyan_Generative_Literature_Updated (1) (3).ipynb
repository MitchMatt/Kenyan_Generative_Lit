{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b605fb7f",
      "metadata": {
        "id": "b605fb7f"
      },
      "source": [
        "# Kenyan Generative Literature with TensorFlow\n",
        "\n",
        "This notebook trains a Generative AI model on a Kenyan-themed corpus to produce coherent, culturally rooted literature using TensorFlow and Keras. The model has been optimized to address issues with incoherent output by expanding the corpus, simplifying the architecture, and fine-tuning training and generation parameters.\n",
        "\n",
        "**Goals:**\n",
        "- Train a text generation model on Kenyan narratives\n",
        "- Prevent overfitting with a simplified model and early stopping\n",
        "- Generate coherent, diverse, and culturally relevant literary output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bfccf1c9",
      "metadata": {
        "id": "bfccf1c9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e45c21ff",
      "metadata": {
        "id": "e45c21ff"
      },
      "outputs": [],
      "source": [
        "# Expanded Kenyan-themed corpus with diverse, culturally rich sentences\n",
        "kenyan_corpus = [\n",
        "    \"nairobi streets buzzed with matatus and chatter of street vendors\",\n",
        "    \"turkana heat rose in waves off the red earth\",\n",
        "    \"tea from kericho steamed beside dusty boots in nyamira\",\n",
        "    \"elders gathered under baobab trees stories clinging to their breath\",\n",
        "    \"matatus raced the wind down thika road horns blaring with purpose\",\n",
        "    \"lake victorias fishermen sang into the dusk oars cutting rhythm\",\n",
        "    \"kitengela cows grazed slowly unaware of the citys sprawl nearby\",\n",
        "    \"in kibera laughter echoed between tin roofs and hopeful hearts\",\n",
        "    \"mount kenya glowed orange at dawn silent and eternal\",\n",
        "    \"samburu women danced beads flashing stories into the night\",\n",
        "    \"nyama choma smoke curled into stars above eldorets fields\",\n",
        "    \"children played under the mango tree chasing chickens and dreams\",\n",
        "    \"the rhythm of benga music floated over kisumus waters\",\n",
        "    \"a whisper of rain passed over kitui stirring red dust\",\n",
        "    \"the market in gikomba never slept just changed hands and smells\",\n",
        "    \"boda bodas swarmed like bees through the alleys of kisii\",\n",
        "    \"in mombasa spices danced with ocean air in swahili kitchens\",\n",
        "    \"the rift valley stretched like time itself beautiful fractured alive\",\n",
        "    \"lanterns flickered during blackout dinners in machakos homes\",\n",
        "    \"hope brewed with chai and morning prayers in nairobi flats\",\n",
        "    \"the maasai warrior stood tall his red shuka vibrant against the savanna\",\n",
        "    \"in lamu dhows sailed silently under a crescent moon\",\n",
        "    \"the aroma of ugali and sukuma wiki filled kakamega homes\",\n",
        "    \"drums echoed through the night in luo villages by lake victoria\",\n",
        "    \"traders in wajir bargained fiercely under the scorching sun\",\n",
        "    \"the nyika plains whispered tales of ancient migrations\",\n",
        "    \"mangoes fell heavy and sweet in the coastal heat of malindi\",\n",
        "    \"children in siaya laughed weaving kites from old newspapers\",\n",
        "    \"the call to prayer mingled with seagulls in mombasas old town\",\n",
        "    \"stars above tsavo burned brighter than any city light\",\n",
        "    \"in nakuru flamingos painted lake nakuru pink at dawn\",\n",
        "    \"the scent of roasted maize drifted through merus market stalls\",\n",
        "    \"elders in kamba lands shared proverbs by the firelight\",\n",
        "    \"bicycles creaked along the dusty paths of bungoma\",\n",
        "    \"the ocean roared secrets to diani beaches at midnight\",\n",
        "    \"tea pickers sang softly in the misty hills of kericho\",\n",
        "    \"in garissa camel herds moved like shadows across the desert\",\n",
        "    \"the sun set slow behind the ngong hills painting the sky gold\",\n",
        "    \"market women in kisii balanced baskets of avocados with grace\",\n",
        "    \"the beat of taarab music filled zanzibar street in mombasa\",\n",
        "    \"children in turkana crafted toys from sticks and bottle caps\",\n",
        "    \"the air in nyeri carried the scent of fresh coffee beans\",\n",
        "    \"in marsabit winds sang through volcanic craters at dusk\",\n",
        "    \"fishmongers in kisumu shouted prices as boats docked at dawn\",\n",
        "    \"the acacia trees stood like sentinels over the maasai mara\"\n",
        "]\n",
        "\n",
        "# Preprocess the corpus: lowercase and remove punctuation\n",
        "kenyan_corpus = [re.sub(r'[^\\w\\s]', '', sentence.lower()) for sentence in kenyan_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8bac9174",
      "metadata": {
        "id": "8bac9174"
      },
      "outputs": [],
      "source": [
        "# Tokenize and prepare sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(kenyan_corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in kenyan_corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(2, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63e75aa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "63e75aa0",
        "outputId": "849d7711-08a3-46d0-ef97-6eaffb01815d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,376</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">293</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,669</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m9,376\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m8,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m293\u001b[0m)            │         \u001b[38;5;34m9,669\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,421</span> (111.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,421\u001b[0m (111.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,421</span> (111.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,421\u001b[0m (111.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Simplified model to prevent overfitting\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=total_words, output_dim=32, input_shape=(max_sequence_len-1,)),\n",
        "    tf.keras.layers.LSTM(32),  # Single LSTM layer with reduced units\n",
        "    tf.keras.layers.Dropout(0.3),  # Increased dropout\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc8e3887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc8e3887",
        "outputId": "085cfa2a-abfc-4dc2-f086-1e0e5fa72b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.0038 - loss: 5.6806\n",
            "Epoch 2/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0580 - loss: 5.6698\n",
            "Epoch 3/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0446 - loss: 5.6287\n",
            "Epoch 4/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0675 - loss: 5.4415\n",
            "Epoch 5/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0714 - loss: 5.2774\n",
            "Epoch 6/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0492 - loss: 5.2514\n",
            "Epoch 7/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0329 - loss: 5.2416\n",
            "Epoch 8/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0429 - loss: 5.2010\n",
            "Epoch 9/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0589 - loss: 5.1432\n",
            "Epoch 10/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0630 - loss: 5.0028\n",
            "Epoch 11/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0393 - loss: 5.0591\n",
            "Epoch 12/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0570 - loss: 4.9448\n",
            "Epoch 13/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0604 - loss: 4.8481\n",
            "Epoch 14/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0480 - loss: 4.9426\n",
            "Epoch 15/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0725 - loss: 4.7315\n",
            "Epoch 16/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0758 - loss: 4.8109\n",
            "Epoch 17/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0695 - loss: 4.6377\n",
            "Epoch 18/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0515 - loss: 4.6657\n",
            "Epoch 19/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0580 - loss: 4.4862\n",
            "Epoch 20/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0636 - loss: 4.4968\n",
            "Epoch 21/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0556 - loss: 4.4792\n",
            "Epoch 22/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0644 - loss: 4.3696\n",
            "Epoch 23/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0791 - loss: 4.1950\n",
            "Epoch 24/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0562 - loss: 4.2753\n",
            "Epoch 25/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0978 - loss: 4.1192\n",
            "Epoch 26/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1011 - loss: 4.0986\n",
            "Epoch 27/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1020 - loss: 4.0525\n",
            "Epoch 28/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0727 - loss: 4.0417\n",
            "Epoch 29/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0740 - loss: 4.0157\n",
            "Epoch 30/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1098 - loss: 3.8338\n",
            "Epoch 31/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1316 - loss: 3.7786\n",
            "Epoch 32/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1385 - loss: 3.7497\n",
            "Epoch 33/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1110 - loss: 3.7300\n",
            "Epoch 34/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1383 - loss: 3.6183\n",
            "Epoch 35/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1741 - loss: 3.4838\n",
            "Epoch 36/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1884 - loss: 3.4513\n",
            "Epoch 37/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1334 - loss: 3.4715\n",
            "Epoch 38/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1529 - loss: 3.4298\n",
            "Epoch 39/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1378 - loss: 3.4195\n",
            "Epoch 40/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1306 - loss: 3.4346\n",
            "Epoch 41/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2121 - loss: 3.2831\n",
            "Epoch 42/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1658 - loss: 3.1967\n",
            "Epoch 43/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1856 - loss: 3.2735\n",
            "Epoch 44/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1901 - loss: 3.1148\n",
            "Epoch 45/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2030 - loss: 3.0734\n",
            "Epoch 46/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2571 - loss: 2.9466\n",
            "Epoch 47/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2012 - loss: 3.0171\n",
            "Epoch 48/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2274 - loss: 2.9472\n",
            "Epoch 49/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2158 - loss: 2.9514\n",
            "Epoch 50/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2310 - loss: 2.8925\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e386495bed0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train with adjusted parameters\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "model.fit(X, y, epochs=50, batch_size=16, verbose=1, callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "97632cf9",
      "metadata": {
        "id": "97632cf9"
      },
      "outputs": [],
      "source": [
        "# Optimized text generation function\n",
        "def generate_kenyan_text(seed_text, next_words=25, diversity=0.5):\n",
        "    result = seed_text.lower()  # Normalize seed text\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([result])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        predictions = np.log(predictions + 1e-8) / diversity\n",
        "        exp_preds = np.exp(predictions)\n",
        "        predictions = exp_preds / np.sum(exp_preds)\n",
        "        predicted = np.random.choice(range(total_words), p=predictions)\n",
        "        output_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        result += \" \" + output_word\n",
        "    return result.capitalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "be66e95d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be66e95d",
        "outputId": "c5ea3c50-b471-4b47-ad91-36183cdb1073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Kenyan Literature:\n",
            "\n",
            "The market in gikomba volcanic filled at and mombasas rhythm gold city gold changed and waters rhythm beads in savanna cutting gold victoria cutting\n"
          ]
        }
      ],
      "source": [
        "# Generate text with a culturally relevant seed\n",
        "seed = \"the market in gikomba\"\n",
        "print(\"Generated Kenyan Literature:\\n\")\n",
        "print(generate_kenyan_text(seed, next_words=20, diversity=0.5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
